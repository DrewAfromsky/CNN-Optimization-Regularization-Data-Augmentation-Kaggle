{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Task 1: Optimization\n",
    "\n",
    "In this task, we introduce several improved stochastic gradient descent (SGD) based optimization methods. Plain/naive SGD is a reasonable method to update neural network parameters. However, to make SGD perform well, one would need to find an appropriate learning rate and a good initial value. Otherwise, the network will get stuck if the learning rate is small, or it will diverge if the learning rate is too large. In reality, since we have no prior knowledge about the training data, it is not trivial to find a good learning rate manually. Also, when the network becomes deeper, for each layer one may need to set a different learning rate. Another common problem is the lack of sufficient training data. This can cause the training to get stuck when using the naive SGD method. These are the limitations of the plain SGD, which are motivators for creating and using improved SGD-based methods. \n",
    "\n",
    "To address the question of **how to set a good learning rate?**, one can rely on adaptive learning rate methods.  Here, you are going to experiment with **SGD with momentum**, **RMSProp**, **Adam** and compare them.\n",
    "All of these optimizers are adaptive learning rate methods. Here are a few useful links: http://ruder.io/optimizing-gradient-descent/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.cifar_utils import load_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR 10\n",
    "\n",
    "Here we use a small dataset with only 2500 samples to simulate the \"lack-of-data\" situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "X_val = X_train[:500,:]\n",
    "y_val = y_train[:500]\n",
    "X_train = X_train[500:2500,:]\n",
    "y_train = y_train[500:2500]\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0).astype(np.float32)\n",
    "X_train = X_train.astype(np.float32) - mean_image\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# We have vectorized the data for you. That is, we flatten the 32×32×3 images into 1×3072 Numpy arrays.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement Optimizers\n",
    "\n",
    "Here we provide an MLP code snippet for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.neuralnets.mlp import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original SGD with learning rate decay (for comparison purpose only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.optimizers import SGDOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, weight_scale=1e-3, l2_reg=0.0)\n",
    "optimizer = SGDOptim()\n",
    "hist_sgd = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=30, batch_size=200, learning_rate=1e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD + Momentum\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **SGDmomentumOptim** in __./utils/optimizers.py__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.optimizers import SGDmomentumOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = SGDmomentumOptim(model, momentum=0.8)\n",
    "hist_sgd_momentum = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                                         num_epoch=30, batch_size=200, learning_rate=1e-2, \n",
    "                                         learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **RMSpropOptim** in ./utils/optimizers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.optimizers import RMSpropOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = RMSpropOptim(model)\n",
    "hist_rmsprop = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                               num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                               learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Edit **AdamOptim** in ./utils/optimizers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.optimizers import AdamOptim\n",
    "\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\n",
    "optimizer = AdamOptim(model)\n",
    "hist_adam = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                            num_epoch=30, batch_size=200, learning_rate=1e-3, \n",
    "                            learning_decay=0.95, verbose=False, record_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Comparison\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Run the following cells, which plot the loss curves of different optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist_sgd, train_acc_hist_sgd, val_acc_hist_sgd = hist_sgd\n",
    "loss_hist_momentum, train_acc_hist_momentum, val_acc_hist_momentum = hist_sgd_momentum\n",
    "loss_hist_rmsprop, train_acc_hist_rmsprop, val_acc_hist_rmsprop = hist_rmsprop\n",
    "loss_hist_adam, train_acc_hist_adam, val_acc_hist_adam = hist_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training error curve of optimizers\n",
    "plt.plot(loss_hist_sgd, label=\"sgd\")\n",
    "plt.plot(loss_hist_momentum, label=\"momentum\")\n",
    "plt.plot(loss_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(loss_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training accuracy curve of optimizers\n",
    "plt.plot(train_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(train_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(train_acc_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(train_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation accuracy curve of optimizers\n",
    "plt.plot(val_acc_hist_sgd, label=\"sgd\")\n",
    "plt.plot(val_acc_hist_momentum, label=\"momentum\")\n",
    "plt.plot(val_acc_hist_rmsprop, label=\"rmsprop\")\n",
    "plt.plot(val_acc_hist_adam, label=\"adam\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">__TODO:__</span> Describe your results, and discuss your understandings of these optimizers, such as their advantages/disadvantages and when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: **[fill in here]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
