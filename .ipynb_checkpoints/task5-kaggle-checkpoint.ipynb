{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Assignment 2- Task 5: Kaggle Open-ended Competition\n",
    "\n",
    "Kaggle is a platform for predictive modelling and analytics competitions in which companies and researchers post data and statisticians and data miners compete to produce the best models for predicting and describing the data.\n",
    "\n",
    "If you don't have a Kaggle account, feel free to join at [www.kaggle.com](https://www.kaggle.com). To let the CAs do the grading more conveniently, please __use Lionmail to join Kaggle__ and __use UNI as your username__.\n",
    "\n",
    "The website for this competition is: \n",
    "https://www.kaggle.com/c/e4040fall2019-assignment-2-task-5\n",
    "\n",
    "You can find detailed description about this in-class competition on the website above. Please read carefully and follow the instructions.\n",
    "\n",
    "<span style=\"color:red\">__TODO__:</span>\n",
    "1. Train a custom model for the bottle dataset classification problem. You are free to use any methods taught in the class or found by yourself on the Internet (ALWAYS provide reference to the source) but __TF 2.0 is not allowed to use in this competition__. General training methods include:\n",
    "    * Dropout\n",
    "    * Batch normalization\n",
    "    * Early stopping\n",
    "    * l1-norm & l2-norm penalization\n",
    "2. You'll be given the test set to generate your predictions (70% public + 30% private, but you don't know which ones are public/private). Achieve 70% accuracy on the public test set. The accuracy will be shown on the public leaderboard once you submit your prediction .csv file. The private leaderboard would be released after the competition. The final ranking would based on that result, not the public one.\n",
    "3. \n",
    "\n",
    "    * Report your results on the Kaggle, for comparison with other students' optimal results (you can do this several times). \n",
    "    * Save your best model, using Github Classroom, at the same time when you submit the homework files into Courseworks. See instructions below. \n",
    "\n",
    "__Hint__: You can start from what you implemented in task 4. Another classic classification model named 'VGG16' can also be easily implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Submission Details:\n",
    "There are three components to reporting the results of this task: \n",
    "\n",
    "**(A) Submission (up to 20 submissions each day) of the .csv prediction file throught the Kaggle platform;**. You should start doing this __VERY early__, so that students can compare their work as they are making progress with model optimization.\n",
    "\n",
    "**(B) Editing and submitting the content of this Jupyter notebook, through Courseworks; **\n",
    "(i) The code for your CNN model and for the training function. The code should be stored in __./ecbm4040/neuralnets/kaggle.py__;\n",
    "(ii) Print out your training process and accuracy __within this notebook__;\n",
    "\n",
    "**(C) Submitting your best CNN model through Github Classroom repo.**\n",
    "\n",
    "**Description of (C):** \n",
    "For this task, we will continue to use Github classroom to save your model for submission. \n",
    "\n",
    "<span style=\"color:red\">__Submission content:__ :</span>\n",
    "(i) In your Assignment 2 submission folder, create a subfolder called __KaggleModel__. Upload your best model with all the data output (for example, __MODEL.data-00000-of-00001, MODEL.meta, MODEL.index__) into the folder. \n",
    "(ii) Remember to delete any intermediate results, **we only want your best model. Do not upload any data files**. The instructors will rerun the uploaded best model and verify against the score which you reported on the Kaggle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from utils.cifar_utils import load_data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Kaggle/kaggle_train_128/train_128/0/\n",
      "./data/Kaggle/kaggle_train_128/train_128/1/\n",
      "./data/Kaggle/kaggle_train_128/train_128/2/\n",
      "./data/Kaggle/kaggle_train_128/train_128/3/\n",
      "./data/Kaggle/kaggle_train_128/train_128/4/\n",
      "(15000, 128, 128, 3)\n",
      "(15000,)\n",
      "(3500, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "train_dir = './data/Kaggle/kaggle_train_128/train_128/'\n",
    "\n",
    "# Training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(5):\n",
    "    subdir = train_dir + str(i) + '/' # folders 0-4\n",
    "    print(subdir)\n",
    "    only_train_files = [f for f in listdir(subdir) if isfile(join(subdir, f))]  # all .png files\n",
    "    for o_train in only_train_files:\n",
    "        im_train = Image.open(subdir + o_train)\n",
    "        X_train.append(np.array(im_train))\n",
    "        im_train.close()\n",
    "        y_train.append(i)\n",
    "\n",
    "test_dir = './data/Kaggle/kaggle_test_128/test_128/'\n",
    "\n",
    "# Test data\n",
    "X_test = []\n",
    "only_test_files = [g for g in listdir(test_dir) if isfile(join(test_dir, g))]  # all .png files\n",
    "for o_test in only_test_files:\n",
    "    im_test = Image.open(test_dir + o_test)\n",
    "    X_test.append(np.array(im_test))\n",
    "    im_test.close()\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (13500, 128, 128, 3)\n",
      "Training labels shape:  (13500,)\n",
      "Validation data shape:  (1500, 128, 128, 3)\n",
      "Validation labels shape:  (1500,)\n",
      "Test data shape:  (3500, 128, 128, 3)\n",
      "Train labels shape (one-hot):  (13500, 5)\n",
      "Validation labels shape (one-hot):  (1500, 5)\n"
     ]
    }
   ],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# Split training data into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data, and reshape it to be RGB size\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "# num_training = 13500\n",
    "# num_validation = 1500\n",
    "\n",
    "# X_val = X_train[-num_validation:, :]\n",
    "# y_val = y_train[-num_validation:]\n",
    "\n",
    "# X_train = X_train[:num_training, :]\n",
    "# y_train = y_train[:num_training]\n",
    "\n",
    "# X_train = X_train.reshape(-1,3,32,32).transpose(0,2,3,1) / 255\n",
    "# X_val = X_val.reshape(-1,3,32,32).transpose(0,2,3,1) / 255\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "\n",
    "y_train_dummy = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_dummy = tf.keras.utils.to_categorical(y_val)\n",
    "print('Train labels shape (one-hot): ', y_train_dummy.shape)\n",
    "print('Validation labels shape (one-hot): ', y_val_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "base_model=MobileNet(weights='imagenet',include_top=False) \n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(5,activation='softmax')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=base_model.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:20]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs = 98 #batch size\n",
    "# bs = 10 #batch size - submission 1\n",
    "bs= 50 # submission 2\n",
    "epc = 25 #number of epoches\n",
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "train_generator = train_datagen.flow(X_train, y_train_dummy, batch_size=bs)\n",
    "step_size_train=train_generator.n//train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "1500/1500 [==============================] - 42s 28ms/sample - loss: 1.5364 - acc: 0.5140\n",
      "1350/1350 [==============================] - 1467s 1s/step - loss: 0.4742 - acc: 0.8607 - val_loss: 1.5364 - val_acc: 0.5140\n",
      "Epoch 2/25\n",
      "1500/1500 [==============================] - 53s 35ms/sample - loss: 1.0802 - acc: 0.6153\n",
      "1350/1350 [==============================] - 1640s 1s/step - loss: 0.2171 - acc: 0.9416 - val_loss: 1.0802 - val_acc: 0.6153\n",
      "Epoch 3/25\n",
      "1500/1500 [==============================] - 60s 40ms/sample - loss: 0.9290 - acc: 0.6767\n",
      "1350/1350 [==============================] - 1641s 1s/step - loss: 0.1996 - acc: 0.9486 - val_loss: 0.9290 - val_acc: 0.6767\n",
      "Epoch 4/25\n",
      "1500/1500 [==============================] - 52s 34ms/sample - loss: 0.8189 - acc: 0.7207\n",
      "1350/1350 [==============================] - 1702s 1s/step - loss: 0.1526 - acc: 0.9612 - val_loss: 0.8189 - val_acc: 0.7207\n",
      "Epoch 5/25\n",
      "1500/1500 [==============================] - 58s 39ms/sample - loss: 0.8689 - acc: 0.7407\n",
      "1350/1350 [==============================] - 1452s 1s/step - loss: 0.1147 - acc: 0.9705 - val_loss: 0.8689 - val_acc: 0.7407\n",
      "Epoch 6/25\n",
      "1500/1500 [==============================] - 41s 28ms/sample - loss: 0.8755 - acc: 0.7493\n",
      "1350/1350 [==============================] - 1377s 1s/step - loss: 0.1114 - acc: 0.9731 - val_loss: 0.8755 - val_acc: 0.7493\n",
      "Epoch 7/25\n",
      "1500/1500 [==============================] - 45s 30ms/sample - loss: 0.8196 - acc: 0.7467\n",
      "1350/1350 [==============================] - 1588s 1s/step - loss: 0.1029 - acc: 0.9755 - val_loss: 0.8196 - val_acc: 0.7467\n",
      "Epoch 8/25\n",
      " 127/1350 [=>............................] - ETA: 25:38 - loss: 0.0693 - acc: 0.9850"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8618e7c8dee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                    validation_data=(X_val, y_val_dummy))\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/Applications/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit_generator(generator = train_generator,\n",
    "                   steps_per_epoch = step_size_train,\n",
    "                   epochs = epc,\n",
    "                   validation_data=(X_val, y_val_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Baseline accuracy: {model.evaluate(X_val, y_val_dummy)[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# from utils.neuralnets.cnn.jupyter_tensorboard import show_graph \n",
    "# tf.reset_default_graph()\n",
    "# with tf.Session() as sess:\n",
    "#     saver = tf.train.import_meta_graph('model/lenet_1572809175.meta')\n",
    "#     graph = tf.get_default_graph()\n",
    "#     show_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate .csv file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code snippet can be used to generate your prediction .csv file.\n",
    "\n",
    "# import csv\n",
    "# with open('predicted.csv','w') as csvfile:\n",
    "#     fieldnames = ['Id','label']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "#     for index,l in enumerate(predicted_values_generated_by_your_model):\n",
    "#         filename = str(index)+'.png'\n",
    "#         label = str(l)\n",
    "#         writer.writerow({'Id': filename, 'label': label})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTF113",
   "language": "python",
   "name": "envtf113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
